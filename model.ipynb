{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **How do you classify the photos into 2 categories?**\n",
    "# - Binary Image Classification using CNN -  \n",
    "If you need to classify the dataset composed of photos that may have 2 features  \n",
    "(e.g. datasets composed of satellite photos that have 2 features(residential area or nonresidential area)),   \n",
    "maybe this memo can help you.\n",
    "  \n",
    "You can replace your datasets with the sample.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1.Prepare libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#CNN\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "#Data Augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#predict\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#other\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.prepare dataset**  \n",
    "Separate the photos into 2 different datasets and place them in folders A and B  \n",
    "You need set your datasets file paths and pixel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dataset\n",
    "real = './data/real/*.jpg'\n",
    "fake = './data/fake/*.jpg'\n",
    "\n",
    "list_real = []\n",
    "list_fake = []\n",
    "\n",
    "n = 50\n",
    "m = 50\n",
    "#A\n",
    "for x in glob.glob(real):\n",
    "    img = Image.open(x)\n",
    "    resize_img = img.resize((n, m))#resize(n,m)\n",
    "    im  = np.array(resize_img)\n",
    "    im = im.reshape(-1)\n",
    "    list_real.append(im/255)\n",
    "#B  \n",
    "for x in glob.glob(fake):\n",
    "    img = Image.open(x)\n",
    "    resize_img = img.resize((n, m))#resize(n,m)\n",
    "    im  = np.array(resize_img)\n",
    "    im = im.reshape(-1)\n",
    "    list_fake.append(im/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7491</th>\n",
       "      <th>7492</th>\n",
       "      <th>7493</th>\n",
       "      <th>7494</th>\n",
       "      <th>7495</th>\n",
       "      <th>7496</th>\n",
       "      <th>7497</th>\n",
       "      <th>7498</th>\n",
       "      <th>7499</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>0.631373</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.286275</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.243137</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.121569</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.368627</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.278431</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.368627</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.368627</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.290196</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.121569</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.172549</td>\n",
       "      <td>0.121569</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.160784</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.113725</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.760784</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.741176</td>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.662745</td>\n",
       "      <td>0.592157</td>\n",
       "      <td>0.521569</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.678431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.286275</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.513725</td>\n",
       "      <td>0.592157</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.474510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.086275</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.172549</td>\n",
       "      <td>0.145098</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.149020</td>\n",
       "      <td>0.243137</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.596078</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.674510</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.662745</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611</th>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.368627</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.415686</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>0.525490</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.349020</td>\n",
       "      <td>0.537255</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.521569</td>\n",
       "      <td>0.380392</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.525490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486275</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.278431</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.349020</td>\n",
       "      <td>0.286275</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.380392</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.368627</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.443137</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.525490</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3614</th>\n",
       "      <td>0.537255</td>\n",
       "      <td>0.407843</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.537255</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.356863</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.631373</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3615</th>\n",
       "      <td>0.486275</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.384314</td>\n",
       "      <td>0.384314</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.454902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.407843</td>\n",
       "      <td>0.521569</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.443137</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3616 rows × 7501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.588235  0.517647  0.541176  0.611765  0.631373  0.635294  0.607843   \n",
       "1     0.368627  0.364706  0.278431  0.364706  0.368627  0.282353  0.368627   \n",
       "2     0.760784  0.729412  0.709804  0.741176  0.701961  0.662745  0.592157   \n",
       "3     0.513725  0.592157  0.647059  0.509804  0.588235  0.639216  0.490196   \n",
       "4     0.223529  0.172549  0.145098  0.235294  0.184314  0.149020  0.243137   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3611  0.360784  0.364706  0.333333  0.364706  0.368627  0.333333  0.372549   \n",
       "3612  0.525490  0.403922  0.349020  0.537255  0.403922  0.360784  0.521569   \n",
       "3613  0.482353  0.419608  0.419608  0.431373  0.360784  0.368627  0.462745   \n",
       "3614  0.537255  0.407843  0.360784  0.537255  0.400000  0.356863  0.529412   \n",
       "3615  0.486275  0.439216  0.435294  0.435294  0.384314  0.384314  0.462745   \n",
       "\n",
       "             7         8         9  ...      7491      7492      7493  \\\n",
       "0     0.623529  0.627451  0.607843  ...  0.435294  0.286275  0.309804   \n",
       "1     0.376471  0.290196  0.364706  ...  0.176471  0.121569  0.133333   \n",
       "2     0.521569  0.462745  0.678431  ...  0.360784  0.337255  0.313725   \n",
       "3     0.576471  0.639216  0.474510  ...  0.023529  0.039216  0.086275   \n",
       "4     0.188235  0.156863  0.254902  ...  0.686275  0.596078  0.462745   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3611  0.372549  0.341176  0.388235  ...  0.388235  0.321569  0.282353   \n",
       "3612  0.380392  0.345098  0.525490  ...  0.486275  0.337255  0.278431   \n",
       "3613  0.388235  0.403922  0.482353  ...  0.505882  0.450980  0.443137   \n",
       "3614  0.392157  0.345098  0.541176  ...  0.658824  0.431373  0.360784   \n",
       "3615  0.400000  0.403922  0.454902  ...  0.470588  0.419608  0.407843   \n",
       "\n",
       "          7494      7495      7496      7497      7498      7499  label  \n",
       "0     0.243137  0.098039  0.121569  0.321569  0.137255  0.168627    0.0  \n",
       "1     0.172549  0.121569  0.129412  0.160784  0.117647  0.113725    0.0  \n",
       "2     0.372549  0.286275  0.301961  0.321569  0.345098  0.337255    0.0  \n",
       "3     0.019608  0.039216  0.082353  0.023529  0.035294  0.082353    0.0  \n",
       "4     0.674510  0.588235  0.458824  0.662745  0.588235  0.450980    0.0  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "3611  0.431373  0.360784  0.325490  0.478431  0.458824  0.415686    1.0  \n",
       "3612  0.482353  0.349020  0.286275  0.490196  0.380392  0.317647    1.0  \n",
       "3613  0.529412  0.478431  0.466667  0.525490  0.482353  0.470588    1.0  \n",
       "3614  0.631373  0.403922  0.333333  0.666667  0.466667  0.392157    1.0  \n",
       "3615  0.521569  0.482353  0.462745  0.505882  0.462745  0.443137    1.0  \n",
       "\n",
       "[3616 rows x 7501 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make dataframe\n",
    "df_A = pd.DataFrame(list_real)\n",
    "df_A['label'] = np.zeros(len(list_real))\n",
    "df_B = pd.DataFrame(list_fake)\n",
    "df_B['label'] = np.ones(len(list_fake))\n",
    "df =  pd.concat([df_A, df_B], axis=0).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make train-valid dataset\n",
    "X = df.drop('label',axis=1) # Features\n",
    "y = df.label #Objective\n",
    "\n",
    "X_ = X.values.reshape(-1, m, n, 3)#(m,n), not (n,m)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_, y, stratify=y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. study CNN model**  \n",
    "You can change the parameters used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model_CNN\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(filters = 64, kernel_size = (2,2), padding = 'Same', activation ='relu', input_shape = (m,n,3)),#(m,n), not (n,m)\n",
    "    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Conv2D(filters = 128, kernel_size = (2,2), padding = 'Same', activation ='relu'),\n",
    "    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Conv2D(filters = 256, kernel_size = (2,2), padding = 'Same', activation ='relu'),\n",
    "    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'Adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['acc']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34/34 [==============================] - 1s 14ms/stepss: 0.\n",
      "31/31 [==============================] - 5s 139ms/step - loss: 0.0609 - acc: 0.9918 - val_loss: 0.1105 - val_acc: 0.9982\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 0s 14ms/stepss: 0.\n",
      "31/31 [==============================] - 4s 127ms/step - loss: 0.0135 - acc: 0.9988 - val_loss: 0.0336 - val_acc: 0.9982\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 1s 16ms/stepss: 0.\n",
      "31/31 [==============================] - 4s 137ms/step - loss: 0.0103 - acc: 0.9988 - val_loss: 0.0285 - val_acc: 0.9982\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 0s 14ms/stepss: 0.\n",
      "31/31 [==============================] - 4s 140ms/step - loss: 0.0123 - acc: 0.9988 - val_loss: 0.0146 - val_acc: 0.9982\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 0s 14ms/stepss: 0.\n",
      "31/31 [==============================] - 4s 136ms/step - loss: 0.0112 - acc: 0.9988 - val_loss: 0.0222 - val_acc: 0.9982\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 0s 14ms/stepss: 0.\n",
      "31/31 [==============================] - 4s 129ms/step - loss: 0.0070 - acc: 0.9988 - val_loss: 0.0110 - val_acc: 0.9982\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 0s 14ms/stepss: 0.\n",
      "31/31 [==============================] - 4s 131ms/step - loss: 0.0051 - acc: 0.9988 - val_loss: 0.0505 - val_acc: 0.9982\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 0s 14ms/stepss: 0.\n",
      "31/31 [==============================] - 4s 137ms/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0105 - val_acc: 0.9982\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 1s 16ms/stepss: \n",
      "31/31 [==============================] - 4s 142ms/step - loss: 0.0067 - acc: 0.9988 - val_loss: 0.0106 - val_acc: 0.9982\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 1s 18ms/stepss\n",
      "31/31 [==============================] - 5s 149ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.0093 - val_acc: 0.9982\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 1s 17ms/stepss: \n",
      "31/31 [==============================] - 5s 162ms/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0220 - val_acc: 0.9982\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 1s 18ms/stepss\n",
      "31/31 [==============================] - 5s 163ms/step - loss: 0.0099 - acc: 0.9988 - val_loss: 0.0091 - val_acc: 0.9982\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 1s 28ms/step\n",
      "31/31 [==============================] - 6s 201ms/step - loss: 0.0046 - acc: 0.9988 - val_loss: 0.0122 - val_acc: 0.9982\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 1s 21ms/step\n",
      "31/31 [==============================] - 9s 280ms/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0094 - val_acc: 0.9982\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 1s 24ms/step\n",
      "31/31 [==============================] - 6s 185ms/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0082 - val_acc: 0.9982\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 1s 23ms/step\n",
      "31/31 [==============================] - 7s 242ms/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0105 - val_acc: 0.9982\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 1s 28ms/step\n",
      "31/31 [==============================] - 6s 196ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.0091 - val_acc: 0.9982\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 1s 17ms/stepss: \n",
      "31/31 [==============================] - 6s 187ms/step - loss: 0.0049 - acc: 0.9988 - val_loss: 0.0082 - val_acc: 0.9982\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 1s 15ms/stepss: 0.\n",
      "31/31 [==============================] - 4s 137ms/step - loss: 0.0064 - acc: 0.9988 - val_loss: 0.0387 - val_acc: 0.9982\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 0s 14ms/stepss: 0.\n",
      "31/31 [==============================] - 4s 130ms/step - loss: 0.0051 - acc: 0.9988 - val_loss: 0.0193 - val_acc: 0.9982\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 15ms/step\n",
      "valid_f1-score-best: 0.0\n",
      "valid_confusion_matrix-best:\n",
      " [[1083    0]\n",
      " [   2    0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#f1-score_callback\n",
    "class F1Callback(Callback):\n",
    "    def __init__(self, model, X_val, y_val):\n",
    "        self.model = model\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.f1s = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        pred = self.model.predict(self.X_val)\n",
    "        f1_val = f1_score(self.y_val, np.round(pred))\n",
    "        self.f1s.append(f1_val)\n",
    "f1cb = F1Callback(model, X_valid, y_valid)\n",
    "\n",
    "#early_stop\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=30,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "#checkpoint\n",
    "cp = ModelCheckpoint(\"weights.hdf5\", monitor=\"val_loss\", verbose=0,\n",
    "                     save_best_only=True, save_weights_only=True)\n",
    "\n",
    "#Data_Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True\n",
    ")\n",
    "\n",
    "batch_size=80\n",
    "\n",
    "#study\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=batch_size, shuffle=True),\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[f1cb, early_stopping, cp],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#plot\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['acc', 'val_acc']].plot(title=\"Accuracy-F1\")\n",
    "plt.plot(np.arange(len(f1cb.f1s)) + 1, np.array(f1cb.f1s), label=\"val_f1\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()\n",
    "\n",
    "#predict validation data with best\n",
    "model.load_weights('./weights.hdf5')\n",
    "y_pred = model.predict(X_valid).round()\n",
    "#valid_score\n",
    "print(\"valid_f1-score-best:\", f1_score(y_valid, y_pred))\n",
    "print(\"valid_confusion_matrix-best:\\n\", confusion_matrix(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. save model**  \n",
    "You need to prepare test dataset with the same way as train-valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
